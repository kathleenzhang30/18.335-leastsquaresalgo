This is the code respository for my 18.335 final project. 

The Levenberg–Marquardt (LM) algorithm is implemented and compared similar algorithms like Gauss–Newton (GN), Gradient Descent (GD), and Stochastic Gradient Descent (SGD) on a nonlinear curve fitting task. Their behavior is compared in terms of convergence dynamics and predictive accuracy relative to ground truth data. Empirically, LM tracks the actual signal most closely and GN also produces similar accurate predictions. Its fit is consistently a bit less precise than LM, particularly near regions where the target curve changes curvature. GD and SGD produce noticeably poorer fits. GD exhibits large, abrupt deviations from the true curve, and SGD shows the same pattern but with smaller magnitude fluctuations due to stochastic averaging. Convergence analysis shows that LM and GN achieve rapid loss reduction, dropping sharply after a small number of iterations. These results highlight the advantages of second order methods for stability and accuracy, and clarify the tradeoffs faced by first order methods in nonlinear regression settings.

There are 4 files in this respository: 
1. lm_implementation.py -- this holds the LM algorithm that I implemented from scratch, as well as GN, GD, and SGD that I got from previous implemented packages.
2. synthetic_data.py -- this holds the code for generating the synthetic data and their models (linear, nonlinear, high dimensional, and oscillatory). It also prints out tables for metrics like MSE and R^2 as well as create graphs plotting how well each of the four algorithmns do.
3. diabetes_data.py -- this is a real life diabetes dataset from scikit-learn. This code tests the four algorithms on a nonlinear neural network model, and also prints out relevant metrics and graphs for convergence/actual vs predicted.
4. temp_data.py -- this is a real life temperature dataset of Boston. This code tests the four algorithms on a oscillatory model. Prints out relevant metrics and graphs for convergence/actual vs predicted.
